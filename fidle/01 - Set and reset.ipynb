{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"800px\" src=\"../fidle/img/00-Fidle-header-01.svg\"></img>\n",
    "\n",
    "\n",
    "## Mise a jour du catalog des notebooks et des READMEs\n",
    " - Génération du **catalog des notebooks** : `./log/catalog_nb.json`\n",
    " - Génération automatique de la **table des matières**\n",
    " - Mise à jour des **README** `README.md` et `REAME.ipynb`\n",
    " - #Reset du **finihed_file**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Load modules and init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "from IPython.display import display,Image,Markdown,HTML\n",
    "import re\n",
    "import sys, os, glob\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append('..')\n",
    "import fidle.pwk as pwk\n",
    "import fidle.config as config\n",
    "import fidle.cooker as cooker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - List of folders containing notebooks to be indexed :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories_to_index = ['LinearReg', 'IRIS', 'BHPD', 'MNIST', 'GTSRB', 'IMDB', 'SYNOP', 'VAE', 'Misc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Catalog of notebooks\n",
    "### 3.1 - Build catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog saved as ../fidle/log/catalog.json\n",
      "Entries :  32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- Get the notebook list\n",
    "#\n",
    "notebooks_list = cooker.get_notebooks(directories_to_index)\n",
    "\n",
    "# ---- Get a detailled catalog for this list\n",
    "#\n",
    "catalog = cooker.get_catalog(notebooks_list)\n",
    "\n",
    "with open(config.CATALOG_FILE,'wt') as fp:\n",
    "    json.dump(catalog,fp,indent=4)\n",
    "    print(f'Catalog saved as {config.CATALOG_FILE}')\n",
    "    print('Entries : ',len(catalog))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 build index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Index is :**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| | |\n",
       "|--|--|\n",
       "|LINR1| [Linear regression with direct resolution](LinearReg/01-Linear-Regression.ipynb)<br>Direct determination of linear regression |\n",
       "|GRAD1| [Linear regression with gradient descent](LinearReg/02-Gradient-descent.ipynb)<br>An example of gradient descent in the simple case of a linear regression.|\n",
       "|POLR1| [Complexity Syndrome](LinearReg/03-Polynomial-Regression.ipynb)<br>Illustration of the problem of complexity with the polynomial regression|\n",
       "|LOGR1| [Logistic regression, with sklearn](LinearReg/04-Logistic-Regression.ipynb)<br>Logistic Regression using Sklearn|\n",
       "|PER57| [Perceptron Model 1957](IRIS/01-Simple-Perceptron.ipynb)<br>A simple perceptron, with the IRIS dataset.|\n",
       "|BHP1| [Regression with a Dense Network (DNN)](BHPD/01-DNN-Regression.ipynb)<br>A Simple regression with a Dense Neural Network (DNN) - BHPD dataset|\n",
       "|BHP2| [Regression with a Dense Network (DNN) - Advanced code](BHPD/02-DNN-Regression-Premium.ipynb)<br>More advanced example of DNN network code - BHPD dataset|\n",
       "|MNIST1| [Simple classification with DNN](MNIST/01-DNN-MNIST.ipynb)<br>Example of classification with a fully connected neural network|\n",
       "|GTS1| [CNN with GTSRB dataset - Data analysis and preparation](GTSRB/01-Preparation-of-data.ipynb)<br>Episode 1 : Data analysis and creation of a usable dataset|\n",
       "|GTS2| [CNN with GTSRB dataset - First convolutions](GTSRB/02-First-convolutions.ipynb)<br>Episode 2 : First convolutions and first results|\n",
       "|GTS3| [CNN with GTSRB dataset - Monitoring ](GTSRB/03-Tracking-and-visualizing.ipynb)<br>Episode 3 : Monitoring and analysing training, managing checkpoints|\n",
       "|GTS4| [CNN with GTSRB dataset - Data augmentation ](GTSRB/04-Data-augmentation.ipynb)<br>Episode 4 : Improving the results with data augmentation|\n",
       "|GTS5| [CNN with GTSRB dataset - Full convolutions ](GTSRB/05-Full-convolutions.ipynb)<br>Episode 5 : A lot of models, a lot of datasets and a lot of results.|\n",
       "|GTS6| [Full convolutions as a batch](GTSRB/06-Notebook-as-a-batch.ipynb)<br>Episode 6 : Run Full convolution notebook as a batch|\n",
       "|GTS7| [CNN with GTSRB dataset - Show reports](GTSRB/07-Show-report.ipynb)<br>Episode 7 : Displaying a jobs report|\n",
       "|TSB1| [Tensorboard with/from Jupyter ](GTSRB/99-Scripts-Tensorboard.ipynb)<br>4 ways to use Tensorboard from the Jupyter environment|\n",
       "|IMDB1| [Text embedding with IMDB](IMDB/01-Embedding-Keras.ipynb)<br>A very classical example of word embedding for text classification (sentiment analysis)|\n",
       "|IMDB2| [Text embedding with IMDB - Reloaded](IMDB/02-Prediction.ipynb)<br>Example of reusing a previously saved model|\n",
       "|IMDB3| [Text embedding/LSTM model with IMDB](IMDB/03-LSTM-Keras.ipynb)<br>Still the same problem, but with a network combining embedding and LSTM|\n",
       "|SYNOP1| [Time series with RNN - Preparation of data](SYNOP/01-Preparation-of-data.ipynb)<br>Episode 1 : Data analysis and creation of a usable dataset|\n",
       "|SYNOP2| [Time series with RNN - Try a prediction](SYNOP/02-First-predictions.ipynb)<br>Episode 2 : Training session and first predictions|\n",
       "|SYNOP3| [Time series with RNN - 12h predictions](SYNOP/03-12h-predictions.ipynb)<br>Episode 3: Attempt to predict in the longer term |\n",
       "|VAE1| [Variational AutoEncoder (VAE) with MNIST](VAE/01-VAE-with-MNIST.nbconvert.ipynb)<br>Episode 1 : Model construction and Training|\n",
       "|VAE2| [Variational AutoEncoder (VAE) with MNIST - Analysis](VAE/02-VAE-with-MNIST-post.ipynb)<br>Episode 2 : Exploring our latent space|\n",
       "|VAE3| [About the CelebA dataset](VAE/03-About-CelebA.ipynb)<br>Episode 3 : About the CelebA dataset, a more fun dataset ;-)|\n",
       "|VAE4| [Preparation of the CelebA dataset](VAE/04-Prepare-CelebA-datasets.ipynb)<br>Episode 4 : Preparation of a clustered dataset, batchable|\n",
       "|VAE5| [Checking the clustered CelebA dataset](VAE/05-Check-CelebA.ipynb)<br>Episode 5 :\tChecking the clustered dataset|\n",
       "|VAE6| [Variational AutoEncoder (VAE) with CelebA (small)](VAE/06-VAE-with-CelebA-s.nbconvert.ipynb)<br>Episode 6 : Variational AutoEncoder (VAE) with CelebA (small res.)|\n",
       "|VAE7| [Variational AutoEncoder (VAE) with CelebA (medium)](VAE/07-VAE-with-CelebA-m.nbconvert.ipynb)<br>Episode 7 : Variational AutoEncoder (VAE) with CelebA (medium res.)|\n",
       "|VAE8| [Variational AutoEncoder (VAE) with CelebA - Analysis](VAE/08-VAE-withCelebA-post.ipynb)<br>Episode 8 : Exploring latent space of our trained models|\n",
       "|ACTF1| [Activation functions](Misc/Activation-Functions.ipynb)<br>Some activation functions, with their derivatives.|\n",
       "|NP1| [A short introduction to Numpy](Misc/Numpy.ipynb)<br>Numpy is an essential tool for the Scientific Python.|"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---- Create a markdown index\n",
    "#\n",
    "lines=['| | |','|--|--|']\n",
    "tab='&nbsp;'*5\n",
    "for id, about in catalog.items():\n",
    "    id          = about['id']\n",
    "    dirname     = about['dirname']\n",
    "    basename    = about['basename']\n",
    "    title       = about['title']\n",
    "    description = about['description']\n",
    "      \n",
    "#     lines.append( f'[[{id}] - {title}]({dirname}/{basename})  ' )\n",
    "#     lines.append( f'{tab}{description}  ')\n",
    "    lines.append( f'|{id}| [{title}]({dirname}/{basename})<br>{description}|')\n",
    "\n",
    "index = '\\n'.join(lines)\n",
    "display(Markdown('**Index is :**'))\n",
    "display(Markdown(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Update README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md is updated.\n"
     ]
    }
   ],
   "source": [
    "# ---- Create a markdown index\n",
    "#\n",
    "lines=['| | |','|--|--|']\n",
    "tab='&nbsp;'*5\n",
    "for id, about in catalog.items():\n",
    "    id          = about['id']\n",
    "    dirname     = about['dirname']\n",
    "    basename    = about['basename']\n",
    "    title       = about['title']\n",
    "    description = about['description']\n",
    "      \n",
    "#     lines.append( f'[[{id}] - {title}]({dirname}/{basename})  ' )\n",
    "#     lines.append( f'{tab}{description}  ')\n",
    "    lines.append( f'|{id}| [{title}]({dirname}/{basename})<br>{description}|')\n",
    "\n",
    "index = '\\n'.join(lines)\n",
    "    \n",
    "# ---- Load README.md\n",
    "#\n",
    "with open('../README.md','r') as fp:\n",
    "    readme=fp.read()\n",
    "    \n",
    "# ---- Update index, version\n",
    "#\n",
    "readme = cooker.tag('INDEX',   index,          readme)\n",
    "readme = cooker.tag('VERSION', config.VERSION, readme)\n",
    "\n",
    "# ---- Save it\n",
    "#\n",
    "with open('../README.md','wt') as fp:\n",
    "    fp.write(readme)\n",
    "\n",
    "print('README.md is updated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - README.ipynb\n",
    "Just execute README.ipynb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ---- Load notebook\n",
    "#\n",
    "notebook = nbformat.read('../README.ipynb', nbformat.NO_CONVERT)\n",
    "\n",
    "# new_cell = nbformat.v4.new_markdown_cell(source=readme)\n",
    "# notebook.cells.append(new_cell)\n",
    "\n",
    "# ---- Execute it\n",
    "#\n",
    "ep = ExecutePreprocessor(timeout=600, kernel_name=\"python3\")\n",
    "ep.preprocess(notebook,  {'metadata': {'path': '..'}})\n",
    "\n",
    "# ---- Save it\n",
    "with open('../READMEv2.ipynb', mode=\"w\", encoding='utf-8') as fp:\n",
    "    nbformat.write(notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - More fun : Create and execute it :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus rigolo, on va fabriquer le README.ipynb et l'executer :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.ipynb built and saved\n"
     ]
    }
   ],
   "source": [
    "# ---- Create Notebook from scratch\n",
    "#\n",
    "notebook = nbformat.v4.new_notebook()\n",
    "\n",
    "# ---- Add a code cell\n",
    "#\n",
    "code = \"from IPython.display import display,Markdown\\n\"\n",
    "code+= \"display(Markdown(open('README.md', 'r').read()))\\n\"\n",
    "code+= \"#\\n\"\n",
    "code+= \"# This README is visible under Jupiter LAb ! :-)\"\n",
    "\n",
    "new_cell = nbformat.v4.new_code_cell(source=code)\n",
    "new_cell['metadata']= { \"jupyter\": { \"source_hidden\": True} }\n",
    "notebook.cells.append(new_cell)\n",
    "\n",
    "# ---- Run it\n",
    "#\n",
    "ep = ExecutePreprocessor(timeout=600, kernel_name=\"python3\")\n",
    "ep.preprocess(notebook,  {'metadata': {'path': '..'}})\n",
    "\n",
    "# ---- Save it\n",
    "#\n",
    "with open('../README.ipynb', mode=\"w\", encoding='utf-8') as fp:\n",
    "    nbformat.write(notebook, fp)\n",
    "print('README.ipynb built and saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Reset Finished_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished file has been reset.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pwk.reset_finished_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<img width=\"80px\" src=\"../fidle/img/00-Fidle-logo-01.svg\"></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
