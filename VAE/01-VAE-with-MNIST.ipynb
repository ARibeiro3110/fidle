{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"800px\" src=\"../fidle/img/00-Fidle-header-01.svg\"></img>\n",
    "\n",
    "# <!-- TITLE --> [VAE1] - Variational AutoEncoder (VAE) with MNIST\n",
    "<!-- DESC --> Episode 1 : Model construction and Training\n",
    "\n",
    "<!-- AUTHOR : Jean-Luc Parouty (CNRS/SIMaP) -->\n",
    "\n",
    "## Objectives :\n",
    " - Understanding and implementing a **variational autoencoder** neurals network (VAE)\n",
    " - Understanding a more **advanced programming model**\n",
    "\n",
    "The calculation needs being important, it is preferable to use a very simple dataset such as MNIST to start with.\n",
    "\n",
    "## What we're going to do :\n",
    "\n",
    " - Defining a VAE model\n",
    " - Build the model\n",
    " - Train it\n",
    " - Follow the learning process with Tensorboard\n",
    "\n",
    "----\n",
    "## Bug Note :\n",
    "**Works in tf 2.0, but not in 2.2/2.3**  \n",
    "\n",
    "See :\n",
    " - https://github.com/tensorflow/tensorflow/issues/34944  \n",
    " - https://github.com/tensorflow/probability/issues/519  \n",
    "\n",
    "Bypass :\n",
    " - Use tf 2.0\n",
    " - Add `tf.config.experimental_run_functions_eagerly(True)` before compilation...  \n",
    "Works fine in versions 2.2, 2.3 but with horrible perf. (7s -> 1'50s)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Init python stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FIDLE 2020 - Variational AutoEncoder (VAE)\n",
      "TensorFlow version   : 2.0.0\n",
      "VAE version          : 1.28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys, importlib\n",
    "\n",
    "import modules.vae\n",
    "import modules.loader_MNIST\n",
    "\n",
    "from modules.vae          import VariationalAutoencoder\n",
    "from modules.loader_MNIST import Loader_MNIST\n",
    "\n",
    "VariationalAutoencoder.about()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded.\n",
      "Normalized.\n",
      "Reshaped to (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = Loader_MNIST.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Get VAE model\n",
    "Nous allons instancier notre modèle VAE.  \n",
    "Ce dernier est défini avec une classe python pour alléger notre code.  \n",
    "La description de nos deux réseaux est donnée en paramètre.  \n",
    "Notre modèle sera sauvegardé dans le dossier : ./run/<tag>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized.\n",
      "Outputs will be in  : ./run/MNIST.001\n",
      "\n",
      " ---------- Encoder -------------------------------------------------- \n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 28, 28, 32)   320         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 14, 14, 64)   18496       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 7, 7, 64)     36928       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 7, 7, 64)     36928       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 3136)         0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mu (Dense)                      (None, 2)            6274        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "log_var (Dense)                 (None, 2)            6274        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "encoder_output (Lambda)         (None, 2)            0           mu[0][0]                         \n",
      "                                                                 log_var[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 105,220\n",
      "Trainable params: 105,220\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      " ---------- Encoder -------------------------------------------------- \n",
      "\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3136)              9408      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 1)         289       \n",
      "=================================================================\n",
      "Total params: 102,017\n",
      "Trainable params: 102,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n",
      "Config saved in     : ./run/MNIST.001/models/vae_config.json\n"
     ]
    }
   ],
   "source": [
    "tag = 'MNIST.001'\n",
    "\n",
    "input_shape = (28,28,1)\n",
    "z_dim       = 2\n",
    "verbose     = 1\n",
    "\n",
    "encoder= [ {'type':'Conv2D',          'filters':32, 'kernel_size':(3,3), 'strides':1, 'padding':'same', 'activation':'relu'},\n",
    "           {'type':'Conv2D',          'filters':64, 'kernel_size':(3,3), 'strides':2, 'padding':'same', 'activation':'relu'},\n",
    "           {'type':'Conv2D',          'filters':64, 'kernel_size':(3,3), 'strides':2, 'padding':'same', 'activation':'relu'},\n",
    "           {'type':'Conv2D',          'filters':64, 'kernel_size':(3,3), 'strides':1, 'padding':'same', 'activation':'relu'}\n",
    "         ]\n",
    "\n",
    "decoder= [ {'type':'Conv2DTranspose', 'filters':64, 'kernel_size':(3,3), 'strides':1, 'padding':'same', 'activation':'relu'},\n",
    "           {'type':'Conv2DTranspose', 'filters':64, 'kernel_size':(3,3), 'strides':2, 'padding':'same', 'activation':'relu'},\n",
    "           {'type':'Conv2DTranspose', 'filters':32, 'kernel_size':(3,3), 'strides':2, 'padding':'same', 'activation':'relu'},\n",
    "           {'type':'Conv2DTranspose', 'filters':1,  'kernel_size':(3,3), 'strides':1, 'padding':'same', 'activation':'sigmoid'}\n",
    "         ]\n",
    "\n",
    "vae = modules.vae.VariationalAutoencoder(input_shape    = input_shape, \n",
    "                                         encoder_layers = encoder, \n",
    "                                         decoder_layers = decoder,\n",
    "                                         z_dim          = z_dim, \n",
    "                                         verbose        = verbose,\n",
    "                                         run_tag        = tag)\n",
    "vae.save(model=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled.\n"
     ]
    }
   ],
   "source": [
    "r_loss_factor = 1000\n",
    "\n",
    "vae.compile( optimizer='adam', r_loss_factor=r_loss_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size        = 100\n",
    "epochs            = 100\n",
    "initial_epoch     = 0\n",
    "k_size            = 1      # 1 mean using 100% of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "  100/60000 [..............................] - ETA: 22:53 - loss: 231.4630 - vae_r_loss: 231.4619 - vae_kl_loss: 0.0012WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.188069). Check your callbacks.\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 63.3781 - vae_r_loss: 60.6616 - vae_kl_loss: 2.7165 - val_loss: 52.9564 - val_vae_r_loss: 49.0133 - val_vae_kl_loss: 3.9432\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 50.9778 - vae_r_loss: 46.8340 - vae_kl_loss: 4.1438 - val_loss: 49.9606 - val_vae_r_loss: 45.9424 - val_vae_kl_loss: 4.0181\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 48.7746 - vae_r_loss: 44.2006 - vae_kl_loss: 4.5740 - val_loss: 48.1807 - val_vae_r_loss: 43.4536 - val_vae_kl_loss: 4.7271\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 47.7099 - vae_r_loss: 42.9376 - vae_kl_loss: 4.7722 - val_loss: 47.5955 - val_vae_r_loss: 42.5357 - val_vae_kl_loss: 5.0598\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 47.1006 - vae_r_loss: 42.2272 - vae_kl_loss: 4.8734 - val_loss: 46.7708 - val_vae_r_loss: 41.8313 - val_vae_kl_loss: 4.9396\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 46.6475 - vae_r_loss: 41.6830 - vae_kl_loss: 4.9645 - val_loss: 46.6976 - val_vae_r_loss: 41.7424 - val_vae_kl_loss: 4.9552\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 46.2664 - vae_r_loss: 41.2206 - vae_kl_loss: 5.0457 - val_loss: 46.3024 - val_vae_r_loss: 41.2648 - val_vae_kl_loss: 5.0376\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 45.9540 - vae_r_loss: 40.8498 - vae_kl_loss: 5.1042 - val_loss: 46.0395 - val_vae_r_loss: 40.8239 - val_vae_kl_loss: 5.2155\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 45.6851 - vae_r_loss: 40.5245 - vae_kl_loss: 5.1605 - val_loss: 45.6592 - val_vae_r_loss: 40.5067 - val_vae_kl_loss: 5.1525\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 45.5136 - vae_r_loss: 40.3106 - vae_kl_loss: 5.2031 - val_loss: 45.8488 - val_vae_r_loss: 40.4984 - val_vae_kl_loss: 5.3503\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 45.2688 - vae_r_loss: 40.0340 - vae_kl_loss: 5.2348 - val_loss: 45.7909 - val_vae_r_loss: 40.3020 - val_vae_kl_loss: 5.4889\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 45.1268 - vae_r_loss: 39.8462 - vae_kl_loss: 5.2805 - val_loss: 45.7002 - val_vae_r_loss: 40.4375 - val_vae_kl_loss: 5.2627\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 44.9566 - vae_r_loss: 39.6502 - vae_kl_loss: 5.3064 - val_loss: 45.3354 - val_vae_r_loss: 39.7202 - val_vae_kl_loss: 5.6152\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 44.8149 - vae_r_loss: 39.4753 - vae_kl_loss: 5.3396 - val_loss: 45.2802 - val_vae_r_loss: 39.9300 - val_vae_kl_loss: 5.3502\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 44.6664 - vae_r_loss: 39.3107 - vae_kl_loss: 5.3557 - val_loss: 45.0225 - val_vae_r_loss: 39.6999 - val_vae_kl_loss: 5.3226\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 44.5010 - vae_r_loss: 39.1082 - vae_kl_loss: 5.3928 - val_loss: 45.0887 - val_vae_r_loss: 39.9217 - val_vae_kl_loss: 5.1670\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 44.3682 - vae_r_loss: 38.9521 - vae_kl_loss: 5.4160 - val_loss: 44.8966 - val_vae_r_loss: 39.5267 - val_vae_kl_loss: 5.3699\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 44.2863 - vae_r_loss: 38.8689 - vae_kl_loss: 5.4174 - val_loss: 45.1149 - val_vae_r_loss: 39.7757 - val_vae_kl_loss: 5.3391\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 44.1897 - vae_r_loss: 38.7458 - vae_kl_loss: 5.4438 - val_loss: 44.9532 - val_vae_r_loss: 39.6497 - val_vae_kl_loss: 5.3035\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 44.0914 - vae_r_loss: 38.6354 - vae_kl_loss: 5.4559 - val_loss: 44.8003 - val_vae_r_loss: 39.6444 - val_vae_kl_loss: 5.1559\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 44.0011 - vae_r_loss: 38.5244 - vae_kl_loss: 5.4768 - val_loss: 44.5087 - val_vae_r_loss: 39.1241 - val_vae_kl_loss: 5.3847\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 43.9031 - vae_r_loss: 38.4095 - vae_kl_loss: 5.4936 - val_loss: 44.6212 - val_vae_r_loss: 39.3066 - val_vae_kl_loss: 5.3145\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 43.8611 - vae_r_loss: 38.3603 - vae_kl_loss: 5.5008 - val_loss: 44.5187 - val_vae_r_loss: 39.1555 - val_vae_kl_loss: 5.3631\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 43.7507 - vae_r_loss: 38.2394 - vae_kl_loss: 5.5113 - val_loss: 44.4635 - val_vae_r_loss: 39.0751 - val_vae_kl_loss: 5.3884\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 43.7050 - vae_r_loss: 38.1710 - vae_kl_loss: 5.5340 - val_loss: 44.6083 - val_vae_r_loss: 39.2665 - val_vae_kl_loss: 5.3418\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 43.5995 - vae_r_loss: 38.0606 - vae_kl_loss: 5.5389 - val_loss: 44.3755 - val_vae_r_loss: 38.8158 - val_vae_kl_loss: 5.5598\n",
      "Epoch 27/100\n",
      "18100/60000 [========>.....................] - ETA: 2s - loss: 43.4844 - vae_r_loss: 37.9106 - vae_kl_loss: 5.5739"
     ]
    }
   ],
   "source": [
    "vae.train(x_train,\n",
    "          x_test,\n",
    "          batch_size        = batch_size, \n",
    "          epochs            = epochs,\n",
    "          initial_epoch     = initial_epoch,\n",
    "          k_size            = k_size\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<img width=\"80px\" src=\"../fidle/img/00-Fidle-logo-01.svg\"></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
