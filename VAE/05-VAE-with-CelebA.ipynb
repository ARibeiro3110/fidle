{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational AutoEncoder (VAE) with CelebA\n",
    "=========================================\n",
    "---\n",
    "Formation Introduction au Deep Learning  (FIDLE) - S. Arias, E. Maldonado, JL. Parouty - CNRS/SARI/DEVLOG - 2020  \n",
    "\n",
    "## Episode 1 - Train a model\n",
    "\n",
    " - Defining a VAE model\n",
    " - Build the model\n",
    " - Train it\n",
    " - Follow the learning process with Tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Setup environment\n",
    "### 1.1 - Python stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "div.warn {    \n",
       "    background-color: #fcf2f2;\n",
       "    border-color: #dFb5b4;\n",
       "    border-left: 5px solid #dfb5b4;\n",
       "    padding: 0.5em;\n",
       "    font-weight: bold;\n",
       "    font-size: 1.1em;;\n",
       "    }\n",
       "\n",
       "\n",
       "\n",
       "div.nota {    \n",
       "    background-color: #DAFFDE;\n",
       "    border-left: 5px solid #92CC99;\n",
       "    padding: 0.5em;\n",
       "    }\n",
       "\n",
       "\n",
       "\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FIDLE 2020 - Practical Work Module\n",
      "Version              : 0.2.8\n",
      "Run time             : Thursday 13 February 2020, 00:18:26\n",
      "TensorFlow version   : 2.0.0\n",
      "Keras version        : 2.2.4-tf\n",
      "\n",
      "FIDLE 2020 - Variational AutoEncoder (VAE)\n",
      "TensorFlow version   : 2.0.0\n",
      "VAE version          : 1.27\n",
      "\n",
      "FIDLE 2020 - DataGenerator\n",
      "Version              : 0.4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os,sys\n",
    "from importlib import reload\n",
    "\n",
    "import modules.vae\n",
    "import modules.data_generator\n",
    "\n",
    "reload(modules.data_generator)\n",
    "reload(modules.vae)\n",
    "\n",
    "from modules.vae  import VariationalAutoencoder\n",
    "from modules.data_generator import DataGenerator\n",
    "\n",
    "sys.path.append('..')\n",
    "import fidle.pwk as ooo\n",
    "reload(ooo)\n",
    "\n",
    "ooo.init()\n",
    "\n",
    "VariationalAutoencoder.about()\n",
    "DataGenerator.about()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - The good place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, we should be at GRICAD !\n",
      "We are going to use: /bettik/PROJECTS/pr-fidle/datasets/celeba\n"
     ]
    }
   ],
   "source": [
    "place, dataset_dir = ooo.good_place( { 'GRICAD' : f'{os.getenv(\"SCRATCH_DIR\",\"\")}/PROJECTS/pr-fidle/datasets/celeba',\n",
    "                                       'IDRIS'  : f'{os.getenv(\"WORK\",\"\")}/datasets/celeba'    } )\n",
    "\n",
    "# ---- train/test datasets\n",
    "\n",
    "train_dir    = f'{dataset_dir}/clusters.train'\n",
    "test_dir     = f'{dataset_dir}/clusters.test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - DataGenerator and validation data\n",
    "Ok, everything's perfect, now let's instantiate our generator for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generator : 6250 batchs of 32 images, or 200000 images\n",
      "x_test : 2599 images\n"
     ]
    }
   ],
   "source": [
    "data_gen = DataGenerator(train_dir, 32, k_size=1)\n",
    "x_test   = np.load(f'{test_dir}/images-000.npy')\n",
    "\n",
    "print(f'Data generator : {len(data_gen)} batchs of {data_gen.batch_size} images, or {data_gen.dataset_size} images')\n",
    "print(f'x_test : {len(x_test)} images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Get VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized.\n",
      "Outputs will be in  : ./run/CelebA.004\n",
      "\n",
      " ---------- Encoder -------------------------------------------------- \n",
      "\n",
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 240, 160, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 120, 80, 32)  896         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 120, 80, 32)  0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 60, 40, 64)   18496       dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 60, 40, 64)   0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 30, 20, 64)   36928       dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 30, 20, 64)   0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 15, 10, 64)   36928       dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 15, 10, 64)   0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 9600)         0           dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mu (Dense)                      (None, 200)          1920200     flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "log_var (Dense)                 (None, 200)          1920200     flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_output (Lambda)         (None, 200)          0           mu[0][0]                         \n",
      "                                                                 log_var[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,933,648\n",
      "Trainable params: 3,933,648\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      " ---------- Encoder -------------------------------------------------- \n",
      "\n",
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 9600)              1929600   \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 15, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_20 (Conv2DT (None, 30, 20, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 30, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_21 (Conv2DT (None, 60, 40, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 60, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_22 (Conv2DT (None, 120, 80, 32)       18464     \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 120, 80, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_23 (Conv2DT (None, 240, 160, 3)       867       \n",
      "=================================================================\n",
      "Total params: 2,022,787\n",
      "Trainable params: 2,022,787\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Config saved in     : ./run/CelebA.004/models/vae_config.json\n"
     ]
    }
   ],
   "source": [
    "tag = 'CelebA.004'\n",
    "\n",
    "input_shape = (128, 128, 3)\n",
    "z_dim       = 200\n",
    "verbose     = 1\n",
    "\n",
    "encoder= [ {'type':'Conv2D',          'filters':32, 'kernel_size':(3,3), 'strides':2, 'padding':'same', 'activation':'relu'},\n",
    "           {'type':'Dropout',         'rate':0.25},\n",
    "           {'type':'Conv2D',          'filters':64, 'kernel_size':(3,3), 'strides':2, 'padding':'same', 'activation':'relu'},\n",
    "           {'type':'Dropout',         'rate':0.25},\n",
    "           {'type':'Conv2D',          'filters':64, 'kernel_size':(3,3), 'strides':2, 'padding':'same', 'activation':'relu'},\n",
    "           {'type':'Dropout',         'rate':0.25},\n",
    "           {'type':'Conv2D',          'filters':64, 'kernel_size':(3,3), 'strides':2, 'padding':'same', 'activation':'relu'},\n",
    "           {'type':'Dropout',         'rate':0.25},\n",
    "         ]\n",
    "\n",
    "decoder= [ {'type':'Conv2DTranspose', 'filters':64, 'kernel_size':(3,3), 'strides':2, 'padding':'same', 'activation':'relu'},\n",
    "           {'type':'Dropout',         'rate':0.25},\n",
    "           {'type':'Conv2DTranspose', 'filters':64, 'kernel_size':(3,3), 'strides':2, 'padding':'same', 'activation':'relu'},\n",
    "           {'type':'Dropout',         'rate':0.25},\n",
    "           {'type':'Conv2DTranspose', 'filters':32, 'kernel_size':(3,3), 'strides':2, 'padding':'same', 'activation':'relu'},\n",
    "           {'type':'Dropout',         'rate':0.25},\n",
    "           {'type':'Conv2DTranspose', 'filters':3,  'kernel_size':(3,3), 'strides':2, 'padding':'same', 'activation':'sigmoid'}\n",
    "         ]\n",
    "\n",
    "vae = modules.vae.VariationalAutoencoder(input_shape    = input_shape, \n",
    "                                         encoder_layers = encoder, \n",
    "                                         decoder_layers = decoder,\n",
    "                                         z_dim          = z_dim, \n",
    "                                         verbose        = verbose,\n",
    "                                         run_tag        = tag)\n",
    "vae.save(model=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled.\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "# optimizer     = 'adam'\n",
    "r_loss_factor = 10000\n",
    "\n",
    "vae.compile(optimizer, r_loss_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Train\n",
    "For 10 epochs, adam optimizer :  \n",
    "- Run time at IDRIS : 1299.77 sec. - 0:21:39\n",
    "- Run time at GRICAD : 2092.77 sec. - 0:34:52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs            = 10\n",
    "initial_epoch     = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   1/6250 [..............................] - ETA: 14:52:19 - loss: 934.0418 - vae_r_loss: 931.0808 - vae_kl_loss: 2.9609WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.274111). Check your callbacks.\n",
      "6250/6250 [==============================] - 245s 39ms/step - loss: 270.7260 - vae_r_loss: 218.7500 - vae_kl_loss: 51.9758 - val_loss: 218.8331 - val_vae_r_loss: 166.5691 - val_vae_kl_loss: 52.0549\n",
      "Epoch 2/10\n",
      "6250/6250 [==============================] - 197s 32ms/step - loss: 225.6190 - vae_r_loss: 170.7332 - vae_kl_loss: 54.8861 - val_loss: 207.9924 - val_vae_r_loss: 154.9203 - val_vae_kl_loss: 52.8327\n",
      "Epoch 3/10\n",
      "6250/6250 [==============================] - 213s 34ms/step - loss: 221.2871 - vae_r_loss: 166.0321 - vae_kl_loss: 55.2546 - val_loss: 205.8002 - val_vae_r_loss: 151.2378 - val_vae_kl_loss: 54.3242\n",
      "Epoch 4/10\n",
      "6250/6250 [==============================] - 209s 33ms/step - loss: 219.1529 - vae_r_loss: 163.8348 - vae_kl_loss: 55.3184 - val_loss: 207.8092 - val_vae_r_loss: 156.0968 - val_vae_kl_loss: 51.4896\n",
      "Epoch 5/10\n",
      "6250/6250 [==============================] - 204s 33ms/step - loss: 217.8949 - vae_r_loss: 162.5502 - vae_kl_loss: 55.3446 - val_loss: 201.9843 - val_vae_r_loss: 147.6789 - val_vae_kl_loss: 54.0625\n",
      "Epoch 6/10\n",
      "6250/6250 [==============================] - 204s 33ms/step - loss: 217.0177 - vae_r_loss: 161.6615 - vae_kl_loss: 55.3561 - val_loss: 203.5926 - val_vae_r_loss: 149.6682 - val_vae_kl_loss: 53.6851\n",
      "Epoch 7/10\n",
      "6250/6250 [==============================] - 208s 33ms/step - loss: 216.3389 - vae_r_loss: 160.9570 - vae_kl_loss: 55.3816 - val_loss: 205.3906 - val_vae_r_loss: 151.8084 - val_vae_kl_loss: 53.3464\n",
      "Epoch 8/10\n",
      "6250/6250 [==============================] - 203s 32ms/step - loss: 215.8649 - vae_r_loss: 160.4855 - vae_kl_loss: 55.3795 - val_loss: 203.8759 - val_vae_r_loss: 150.8492 - val_vae_kl_loss: 52.7782\n",
      "Epoch 9/10\n",
      "6250/6250 [==============================] - 198s 32ms/step - loss: 215.4183 - vae_r_loss: 160.0533 - vae_kl_loss: 55.3648 - val_loss: 205.8977 - val_vae_r_loss: 152.0388 - val_vae_kl_loss: 53.5802\n",
      "Epoch 10/10\n",
      "6250/6250 [==============================] - 213s 34ms/step - loss: 215.0948 - vae_r_loss: 159.6998 - vae_kl_loss: 55.3952 - val_loss: 202.0866 - val_vae_r_loss: 147.1687 - val_vae_kl_loss: 54.5898\n",
      "\n",
      "Train duration : 2092.77 sec. - 0:34:52\n"
     ]
    }
   ],
   "source": [
    "vae.train(data_generator    = data_gen,\n",
    "          x_test            = x_test,\n",
    "          epochs            = epochs,\n",
    "          initial_epoch     = initial_epoch\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
